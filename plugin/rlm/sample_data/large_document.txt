# Sample Document for RLM Testing

## Introduction
This is a sample document demonstrating the Recursive Language Model (RLM) pattern.
RLM allows processing of documents far exceeding typical LLM context windows.

## Overview
The RLM pattern treats long prompts as objects in an external, programmable environment.
Instead of inputting the entire document into the LLM's context window, the LLM:

1. Inspects the document structure
2. Searches for relevant sections
3. Chunks and processes recursively
4. Synthesizes the final answer

## Key Features

### Programmable Context
- CONTEXT variable holds the full document
- LLM writes Python code to explore it
- No need to fit everything in context window

### Recursive Processing
- Use llm_query() to process subsections
- Each recursive call handles manageable chunks
- Results are aggregated at the top level

### Security
- Sandboxed execution environment
- Blocked dangerous operations
- Restricted built-in functions

## Architecture

The RLM system consists of:

1. **RLM Orchestrator**: Manages iteration loop and parses code blocks
2. **Root LLM**: Receives query and generates exploration code
3. **REPL Environment**: Executes code with CONTEXT variable
4. **Recursion**: Sub-LLM calls via llm_query() function

## Benefits

### Handles Massive Documents
Process documents with millions of tokens, far beyond standard context windows.

### Adaptive Exploration
LLM decides how to explore based on the task, not a fixed retrieval strategy.

### Cost Efficient
Only processes relevant sections, reducing token usage compared to long-context models.

### Flexible
Works with any LLM backend - Anthropic, OpenAI, Ollama, local models, etc.

## Comparison to Other Approaches

### vs. RAG (Retrieval-Augmented Generation)
RAG retrieves and inserts passages. RLM treats the entire context as programmable,
deciding dynamically how to break down and explore the input.

### vs. Long-Context Models
Long-context models extend the window but still have limits. RLM is agnostic to
context size, handling arbitrarily large inputs through decomposition.

## Conclusion
RLM represents a new inference paradigm for language models, enabling processing
of contexts far beyond traditional limits through programmatic exploration and
recursive decomposition.

## References
- arXiv:2512.24601 - Recursive Language Models
- MIT CSAIL OASYS Lab
- Alex L. Zhang, Tim Kraska, and Omar Khattab
